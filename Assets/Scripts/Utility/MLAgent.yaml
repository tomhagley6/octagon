behaviors:
  MLAgent:
    trainer_type: ppo
    max_steps: 500000
    #max_steps: 0              # Total training steps
    time_horizon: 64             # How many steps of experience per agent before updating
    summary_freq: 10000          # How often stats are written to TensorBoard
    keep_checkpoints: 5
    checkpoint_interval: 50000

    hyperparameters:
      batch_size: 1024           # Number of experiences used per training batch
      buffer_size: 10240         # How many total experiences are collected before training
      learning_rate: 3.0e-4
      beta: 5.0e-3
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3

    network_settings:
      normalize: true           # Set true if your observations are not normalized (e.g. pixel/camera input)
      hidden_units: 256          # Size of each hidden layer
      num_layers: 2              # How deep the network is
      vis_encode_type: simple    # Visual encoder type: simple, nature_cnn, resnet

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0

    self_play:
      window: 10
      play_against_latest_model_rationale: 0.5
      save_steps: 10000     # How often to save self-play models
      swap_steps: 5000      # How often to swap self-play models
      team_change: 5000  # How often to change teams in self-play

    # Optional for camera input; auto-detected if CameraSensorComponent exists
    # use_visual: true
